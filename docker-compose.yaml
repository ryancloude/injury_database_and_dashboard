# docker-compose.yaml

x-airflow-common: &airflow-common
  image: apache/airflow:2.8.1-python3.9
  env_file:
    - .env
  environment:
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW_URL}
    AIRFLOW__CORE__LOAD_EXAMPLES: "False"
    AIRFLOW__API__AUTH_BACKENDS: "airflow.api.auth.backend.basic_auth"
    PIP_ADDITIONAL_REQUIREMENTS: |
      apache-airflow-providers-docker
    # Expose APP_ENV inside Airflow containers; default to "local" if not set
    APP_ENV: ${APP_ENV:-local}
  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/logs:/opt/airflow/logs
    - ./airflow/plugins:/opt/airflow/plugins
    - /var/run/docker.sock:/var/run/docker.sock
  user: "${AIRFLOW_UID:-50000}:0"
  depends_on:
    - db
  networks:
    - airflow_net

services:
  # ---------------------------------------------------------------------------
  # Local Postgres for Airflow metadata (and optional local baseball sandbox)
  # ---------------------------------------------------------------------------
  db:
    image: postgres:15
    container_name: db
    env_file:
      - .env
    environment:
      POSTGRES_USER: ${LOCAL_PG_USER}
      POSTGRES_PASSWORD: ${LOCAL_PG_PASSWORD}
      POSTGRES_DB: airflow
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    networks:
      - airflow_net

  # ---------------------------------------------------------------------------
  # Airflow init (run once to init DB and create admin user)
  # ---------------------------------------------------------------------------
  airflow-init:
    <<: *airflow-common
    container_name: airflow-init
    entrypoint: ["bash", "-c"]
    command: >
      airflow db init &&
      airflow users create
        --username admin
        --password admin
        --firstname Admin
        --lastname User
        --role Admin
        --email admin@example.com
    restart: "no"

  # ---------------------------------------------------------------------------
  # Airflow webserver
  # ---------------------------------------------------------------------------
  airflow-webserver:
    <<: *airflow-common
    container_name: airflow-webserver
    command: webserver
    ports:
      - "8080:8080"
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # Airflow scheduler
  # ---------------------------------------------------------------------------
  airflow-scheduler:
    <<: *airflow-common
    container_name: airflow-scheduler
    command: scheduler
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # Pipeline app (your Python scripts, run via docker exec)
  # ---------------------------------------------------------------------------
  pipeline-app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: pipeline-app
    env_file:
      - .env
    depends_on:
      - db
    volumes:
      - ./scripts:/app/scripts
      - ./projections:/app/projections
    networks:
      - airflow_net
    # Keep container alive so you can exec into it
    command: tail -f /dev/null

  # ---------------------------------------------------------------------------
  # Streamlit dashboard
  # ---------------------------------------------------------------------------
  dashboard:
    build:
      context: .
      dockerfile: Dockerfile.dashboard
    container_name: dashboard
    env_file:
      - .env
    environment:
      # DSNs handled in code via APP_ENV; this is just a Streamlit setting
      STREAMLIT_BROWSER_GATHER_USAGE_STATS: "false"
    depends_on:
      - db
    ports:
      - "8501:8501"
    networks:
      - airflow_net
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # dbt container
  # ---------------------------------------------------------------------------
  dbt:
    image: ${DBT_IMAGE:-ghcr.io/dbt-labs/dbt-postgres:1.9.latest}
    container_name: dbt
    env_file:
      - .env
    environment:
      DBT_PROFILES_DIR: /root/.dbt
    volumes:
      - ./dbt:/usr/app
      - ./dbt/profiles:/root/.dbt
    working_dir: /usr/app
    networks:
      - airflow_net

volumes:
  postgres_data:

networks:
  airflow_net:
    name: airflow_airflow_net
    driver: bridge